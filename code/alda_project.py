# -*- coding: utf-8 -*-
"""ALDA PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zadEzTTcvg53Y2r8C2aSMQ3bzS_sK9jG
"""

# Download the libraries if you don't have
!pip install nltk
!pip install textblob
!pip install wordcloud
!pip install demoji
!pip install transformers
!pip install tensorflow
!pip install sentencepiece

# Basic Libraries 
# --------------------------------------
import numpy as np
import pandas as pd
import seaborn as sns
# import os
import re
import sys
import demoji
import json
import string
from collections import Counter

# Plot library 
# --------------------------------------
import matplotlib.pyplot as plt
import plotly.graph_objs as go
import plotly.io as pio


# NLP
# --------------------------------------
import nltk
from nltk.corpus import stopwords
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.stem import WordNetLemmatizer
from textblob import Word, TextBlob
from wordcloud import WordCloud, STOPWORDS  # visualization of words
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline


# Metrics 
# --------------------------------------
from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split
from sklearn.preprocessing import LabelEncoder


# Tensorflow
import tensorflow as tf


# Torch & BERT
# --------------------------------------
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from torch.utils.data import TensorDataset, DataLoader
from transformers import BertTokenizer, TFBertForSequenceClassification, TFXLNetModel, XLNetTokenizer
from transformers import BertModel, InputExample, InputFeatures


# Machine Learning Models 
# --------------------------------------
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import plot_confusion_matrix
from sklearn import metrics


# Customize to Remove Warnings and Better Observation 
# --------------------------------------------------------
from warnings import filterwarnings
filterwarnings('ignore')
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 200)
pd.set_option('display.float_format', lambda x: '%.2f' % x)

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('vader_lexicon')
nltk.download('punkt')

"""In this notebook we will perform sentiment analysis on tweets about COVID-19 vaccines

# 1. Load the data
"""

raw_dataset = pd.read_csv("vax_tweets_sentiment.csv")
raw_dataset.head()

"""## 1.1 Explore the data"""

raw_dataset.shape

raw_dataset.info()

raw_dataset = raw_dataset.drop(['id'], axis=1)

raw_dataset.head()

raw_dataset.describe()

# check for missing values
raw_dataset.isnull().sum()

# Function to check the missing data
def missing_data(data):
    total = data.isnull().sum()
    null_percentage = (data.isnull().sum()/data.isnull().count()*100)
    missing_info_data = pd.concat([total, null_percentage], axis=1, keys=['Total', 'Null percentage(%)'])
    types = []
    for col in data.columns:
        dtype = str(data[col].dtype)
        types.append(dtype)
    missing_info_data['Types'] = types
    return(np.transpose(missing_info_data))

missing_data(raw_dataset)

raw_dataset.count()

# Function to check the unique values in each column
def unique_values(data):
    total = data.count()
    unique_info_data = pd.DataFrame(total)
    unique_info_data.columns = ['Total']
    uniques = []
    types = []
    
    for col in data.columns:
        unique = data[col].nunique()
        uniques.append(unique)
        
        dtype = str(data[col].dtype)
        types.append(dtype)
        
    unique_info_data['Uniques'] = uniques
    unique_info_data['Types'] = types
    return(np.transpose(unique_info_data))

unique_values(raw_dataset)

# Function to check the most frequent values in the dataset
def most_frequent_values(data):
    total = data.count()
    most_frequent_info_data = pd.DataFrame(total)
    most_frequent_info_data.columns = ['Total']
    items = []
    vals = []
    for col in data.columns:
        item = data[col].value_counts().index[0]
        val = data[col].value_counts().values[0]
        items.append(item)
        vals.append(val)
    most_frequent_info_data['Most frequent item'] = items
    most_frequent_info_data['Frequence'] = vals
    most_frequent_info_data['Percent from total'] = np.round(vals / total * 100, 3)
    return(np.transpose(most_frequent_info_data))

most_frequent_values(raw_dataset)

# Function to plot the count of values in each columns 
def plot_count(feature, title, df, size=1, ordered=True):
    fig, ax = plt.subplots(1,1, figsize=(4*size,4))
    total = float(len(df))
    if ordered:
        g = sns.countplot(df[feature], order=df[feature].value_counts().index[:20], palette='Set3')
    else:
        g = sns.countplot(df[feature], palette='Set3')
        
    g.set_title("Number and percentage of {}".format(title))
    
    if(size > 2):
        plt.xticks(rotation=90, size=8)
        
    for p in ax.patches:
        height = p.get_height()
        ax.text(p.get_x()+p.get_width()/2., height, '{:1.2f}%'.format(100*height/total), ha="center") 
    plt.show()

plot_count("user_name", "User name", raw_dataset, 4)

plot_count("user_location", "User location", raw_dataset, 4)

plot_count("source", "Source", raw_dataset, 4)

# Function to print the wordcloud
def show_wordcloud(data, title=""):
    text = " ".join(t for t in data.dropna())
    stopwords = set(STOPWORDS)
    stopwords.update(["t", "co", "https", "amp", "U"])
    wordcloud = WordCloud(stopwords=stopwords, scale=4, max_font_size=50, max_words=500,background_color="white").generate(text)
    fig = plt.figure(1, figsize=(16,16))
    plt.axis('off')
    fig.suptitle(title, fontsize=20)
    fig.subplots_adjust(top=2.3)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.show()

"""### Prevalent words in tweet texts"""

show_wordcloud(raw_dataset['text'], title = 'Prevalent words in tweets')

"""# 2. Data Cleaning"""

# get the mapping of emojis and its meaning
def get_emojis(s):
  emoji_map.update(demoji.findall(s))

# df.text.apply(get_emojis)
# with open("emoji_mapping.json", "w") as outfile:
#     json.dump(emoji_map, outfile)
  
with open("emoji_mapping.json", "r") as infile:
    emoji_map = json.load(infile)

def tweets_cleaning(df, text_col="text", emoji_map={}):
  
  # demojify
  def replace_emoji(s):
    # print(s)
    if s is None:
      return ""
    if s is np.nan:
      return ""
    for word, replacement in emoji_map.items():
      s = s.replace(word, " "+replacement+" ")
    return s

  df["original_text"] = df[text_col].copy()
  df[text_col] = df[text_col].apply(replace_emoji)


  # Remove twitter handles
  df[text_col] = df[text_col].apply(lambda x:re.sub('@[^\s]+','',str(x)))
    
  # Remove URLs
  df[text_col] = df[text_col].apply(lambda x:re.sub(r"http\S+", "", str(x)))

  # Remove hashtags
  df[text_col] = df[text_col].apply(lambda x:re.sub(r'\B#\S+','',str(x)))

  # Lower casing
  df[text_col] = df[text_col].str.lower()

  # remove numbers and special characters
  df[text_col] = df[text_col].apply(lambda x : re.sub(r"[^a-zA-Z ]", "", x))

  df = df[df[text_col].apply(lambda x : len(x))>2]
  
  # Remove stopwords
  def remove_stopwords(text):
    STOPWORDS = set(stopwords.words('english'))
    update_words = ['covid','#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', 
                    '#covid19', '#covid_19', '#epitwitter', '#ihavecorona', 'amp', 'coronavirus', 'covid19']
    STOPWORDS.update(update_words)
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])

  def lemmatize(text):
    word_list = nltk.word_tokenize(text)
    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
    if text!=lemmatized_output:
      print(text,"===",lemmatized_output)
    return lemmatized_output

  
  df[text_col] = df[text_col].apply(lambda text: remove_stopwords(text))

  #df[text_col] = df[text_col].apply(lambda text: lemmatize(text))

  # Single character or double space removal
  df[text_col] = df[text_col].apply(lambda x:re.sub(r'\s+[a-zA-Z]\s+', '', str(x)))
  df[text_col] = df[text_col].apply(lambda x:re.sub(r'\s+', ' ', str(x), flags=re.I))

  return df[df[text_col]!=''] # remove none

df = tweets_cleaning(raw_dataset, emoji_map=emoji_map)

df.head()

"""# 3. Visualization

We will make some visualizations for exploratory data analysis. It is not for modeling, it is for observation and knowing data better.

Calculation of Term Frequencies
We can use Bar Plot, WordCloud or visualization techniques that can be used for categorical features by obtaining a numeric value.
"""

# TLE
# extract the term frequencies(frequency of the words) and create a df
# tf = df["text"].apply(lambda x: pd.value_counts(x.split(" "))).sum(axis=0).reset_index()
# tf.head()

# Above, we have a way to observe, not in terms of modeling, but in terms of exploring and analyzing.

"""## 3.1 WordCloud

It allows to creation cloud shaped visuals according to the frequency of the words in the relevant text.<br>
For this, we need to specify all the texts as a single text. So, we have to make a single line to all elements of text column.
"""

# check every row and join with these with a space
text = " ".join(i for i in df.text)

wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

# configure the graph
wordcloud = WordCloud(max_font_size=50,
                      max_words=100,
                      background_color="white").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""# 4. Sentiment Analysis

This section will be divided into 2 parts. The first part is to explain methods and how to use sentiment analysis. The second part is to implementation of sentiment analysis in our dataset.

1. Sentiment Analysis Tutorial
2. Implementation to Our Dataset 

## 4.1 Sentiment Analysis Tutorial

Sentiment Analysis aims to express the emotional state of the texts in a mathematical way. Let's say we have a sentence, each word in it has positive/negative/neutral meanings. These meanings are evaluated holistically and evaluations are made about whether a text is positive or negative.
"""

df["text"].head()

"""For instance, "good" has a negative meaning here. There is a pre-trained model in nltk for the meanings of words that can carry negative / positive values in this way. Such models are called ***pretty rate models***. Thanks to this pretty rate model, we find existing word sets and scores. These scores are called ***polarity scores***."""

# Example 1
sia = SentimentIntensityAnalyzer()
sia.polarity_scores("The covid situation was bad")

"""OUR FOCUS IS <u>THE COMPOUND SCORE</u>. 
<br>
Scores from the Compound Score are between -1/1. If the score is less than 0, the text is negative; if it is higher than 0, it means the text is positive.
"""

df["text"][0:10].apply(lambda x: sia.polarity_scores(x))

# take only compound scores
df["text"][0:10].apply(lambda x: sia.polarity_scores(x)["compound"])

# save the scores as a new column
df["polarity_score"] = df["text"].apply(lambda x: sia.polarity_scores(x)["compound"])

"""# 5. Feature Engineering

Firstly, we have to understand how to convert an unsupervised learning problem to a supervised learning problem.<br>

Let's say we have an unsupervised learning method. To make this supervised, we create labels (like positive-negative). Then, we choose the labels we created as the target variable, so that when a new data comes in, we get which class it belongs to as a result of the model.

We can look at the model building phase as starting here because we will create a label and select it as the target variable, and now we will have a supervised learning question.

Let's start with an example, then, apply it to all variables on text column.
"""

# we have such values and we will create a new variable by taking all of them so that we have a label.
df["text"][0:10].apply(lambda x: 'Negative' if sia.polarity_scores(x)["compound"] < 0 else ('Neutral' if sia.polarity_scores(x)["compound"]==0 else 'Positive'))

# if we want to see this example with new label and pos/neg side by side
rev_pol = pd.concat([df["text"][0:10], df["text"][0:10].apply(lambda x: 'Negative' if sia.polarity_scores(x)["compound"] < 0 else ('Neutral' if sia.polarity_scores(x)["compound"]==0 else 'Positive'))], axis=1)
rev_pol.columns = ["text", "Polarity Scores"]
rev_pol

"""Let's assign a new variable with a polarity score greater than 0 to be positive and the smaller ones to be negative, and equal to zero to be Neutral."""

df["sentiment_label"] = df["text"].apply(lambda x: 'Negative' if sia.polarity_scores(x)["compound"] < 0 else ('Neutral' if sia.polarity_scores(x)["compound"]==0 else 'Positive'))
df["sentiment_label"].value_counts()

"""Our label has a string naming as pos/neg which is not binary encoded, let's pass it through label_encoder."""

df["Sentiment"] = LabelEncoder().fit_transform(df["sentiment_label"])
df.head()

"""As you can see, we got arguments in X, but what we have is text and not variable/variables, no measurement values, we need to bring them to measurable format. We need to generate features from these texts, we need to derive features that are measurable and can be put into mathematical operations.

THE MOST CRITICAL POINT OF NLP WORKS IS THE NUMERICAL REPRESENTATIONS OF TEXTS AND WORDS. IN OTHER EXPRESSIONS, THEY ARE THE STUDY OF VECTORING WORDS.

In other words, we need to perform such operations on X so that we can bring it into a measurable format, on which mathematical operations and machine learning modeling can be performed. For this we need to create word vectors. Commonly used methods:

1. Count Vectors
2. TF-IDF
3. Another Word Embeddings Methods(Word2Vec, GloVe, BERT etc.)

We will analyze the implementation of Count Vectors and TF-IDF. Another word embedding methods can be researched and applied with similar preprocess(such as remove punctions, numbers etc.). Each of these methods is the method used by the computer to put these texts into mathematical operations in the world of linear algebra. The text I have is in the form of a text and I have to do something with this text so that it can be processed in the linear algebra world. Let's start to explanation of methods and application on our dataset.

## 5.1 Count Vectors

Count vectors means subtracting the frequencies of words. For example, it is to extract how many times the words in the review are used. So how do we represent these words (words, characters, ngrams).

<table>
  <tr>
    <th>Method</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>Word</td>
    <td>Numerical representations of each word (Ex: numerical, word, each)</td>
  </tr>
  <tr>
    <td>Characters</td>
    <td>Numerical representations of each characters(Ex: n, u, m, e, r etc.)</td>
  </tr>
  <tr>
  <td>Ngam</td>
  <td>Refers to producing features according to word phrases</td>
  </tr>
</table>

Let's make an example for ngrams for better understaing.
"""

a = """ngram is a contiguous sequence of n items from a given sample of text or speech."""
TextBlob(a).ngrams(3)  # For example, let's create a triple ngram

"""Groups a word in 3, moves on to the next word and groups it in 3, etc. So ngram is to break words into chunks with groups.

So, we can count words, characters, ngrams.

For example, we have 4 sentences (1st document, 2nd document etc.), it can be anything we get here, for example 1st tweet, 2nd tweet etc. If we look at our example dataset, we have 1st tweet, 2nd tweet... In short, it refers to different units that we are interested in.
"""

corpus = ['This is the first document.',
          'This document is the second document.',
          'And this is the third one.',
          'Is this the first document?']

# word frequency
vectorizer = CountVectorizer()
X_c = vectorizer.fit_transform(corpus)  # we transformed the corpus with fit_transform
vectorizer.get_feature_names()

X_c.toarray()  # we vectorized each unit

"""It means:
*   First document includes 0 "and", 1 "document", 1 "first" etc.
*   Second document includes 0 "and", 2 "document", 0 "first" etc.
*   Third document includes 1 "and", 0 "document", 0 "first" etc.
*   Fourth document includes 0 "and", 1 "document", 1 "first" etc.

For ngram, we will use an argument as analyzer="word", if we do not enter any argument, the default of CountVectorizer is to make word frequency.
"""

# n-gram
vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))
X_n = vectorizer2.fit_transform(corpus)
vectorizer2.get_feature_names()  # it brought the words one by one above, now he brought them in phrases

X_n.toarray()

"""### Application to our project

Firstly, we will apply word count, then, ngram process will be done.
"""

y = df['Sentiment']
X = df['text']

vectorizer = CountVectorizer()  # default => word count
X_count = vectorizer.fit_transform(X)  # X stands for texts, we only count the frequencies of the words

vectorizer.get_feature_names()[350:360]  # let's look at some features

"""## 5.2 TF-IDF (Term Frequency-Inverse Document Frequency)

It is a normalized, standardized word vector generation method to eliminate some of the biases that the count vector method may reveal. Count Vectorizer will create biases in favor of words with high frequency and against other words. In order to eliminate this and standardize it, the TF-IDF method has been proposed.

HOW IT WORKS: A standardization process is performed on the focus of the frequency of the words in the documents and the frequency of the words in the whole corpus.

The most critical point of NLP studies is the effort to represent words/texts numerically, the work of creating a word vector. The method for this is Count Vector, TF-IDF or another Word Embedding methods(such as Word2Vec, GloVe, BERT which can be considered as more advanced methods).

TF-IDF Steps:
1.   Calculates Count Vectors
2.   Calculate TF (frequency of t term in related document/total number of terms in document) - In other words, we subtract the weight of a particular word in a particular document.
3. Calculate IDF( 1+loge( (total number of documents+1)/(number of documents with t term in it+1) )

<i>In step 2, we focused on the words within each document, and in step 3, we try to consider the impact of terms on all documents.</i>

1. Calculate TF*IDF (we multiply the TF matrix by the vector IDF)
2. Perform L2 Normalization. Find the square root of the sum of the squares of the rows, divide all the cells in the corresponding row by the value you found.

When creating the Count Vector, there was a question whether we are doing this for words, characters, or ngrams. Similarly for TF-IDF, there is the question whether to do it for words, characters, or ngrams. As an example, let's do word and ngram as before.

We will use "corpus" that we created above, again.


"""

# word frequency
tf_idf_word_vectorizer = TfidfVectorizer()  # default = word frequency
# corpus_tf_idf_word = tf_idf_word_vectorizer.fit_transform(corpus)
# tf_idf_word_vectorizer.get_feature_names_out()

# corpus_tf_idf_word.toarray()

"""Same with Count Vectors, each text on corpus were taken and each words' TF-IDF scores were calculated above according to word frequency."""

# ngram
# tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3))
# corpus_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(X)
# tf_idf_ngram_vectorizer.get_feature_names_out()

"""### Application to our project
Since our tweeets are saved on X, we will use same process using X rather than corpus.
"""

# word
# tf_idf_word_vectorizer = TfidfVectorizer()
X_tf_idf_word = tf_idf_word_vectorizer.fit_transform(X)

# ngram
tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3))
X_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(X)

"""Finally, we created all of the features. As a reminder, our features are word frequency according to CountVectorizer and TF-IDF scores for words and ngrams. It means, we create 3 features.

# 6. Sentiment Modelling
## 6.1 Logistic Regression

"""

df3 = df[['text', 'sentiment_label']]

df3.head()

df3.shape

vectorizer1 = CountVectorizer() 
final_features1 = vectorizer1.fit_transform(df3['text']).toarray()
final_features1.shape

from sklearn.feature_selection import SelectKBest, chi2
import pickle

X = df3['text']
Y = df3['sentiment_label']
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)

pipeline = Pipeline([('vect', vectorizer1),
                     ('chi',  SelectKBest(chi2, k=1200)),
                     ('clf', LogisticRegression(random_state=0))])

model = pipeline.fit(X_train, y_train)
with open('LogisticRegression.pickle', 'wb') as f:
    pickle.dump(model, f)

ytest = np.array(y_test)

# confusion matrix and classification report(precision, recall, F1-score)
print(classification_report(ytest, model.predict(X_test)))
print(confusion_matrix(ytest, model.predict(X_test)))

matrix_count_log_1 = confusion_matrix(ytest, model.predict(X_test))
sns.heatmap(matrix_count_log_1, annot=True, fmt="d")
plt.title('Confusion Matrix of Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(ytest, model.predict(X_test)))

vectorizer = TfidfVectorizer(min_df= 3, stop_words="english", sublinear_tf=True, norm='l2', ngram_range=(1, 2))
final_features = vectorizer.fit_transform(df3['text']).toarray()
final_features.shape

from sklearn.feature_selection import SelectKBest, chi2
import pickle

X = df3['text']
Y = df3['sentiment_label']
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)

pipeline = Pipeline([('vect', vectorizer),
                     ('chi',  SelectKBest(chi2, k=1200)),
                     ('clf', LogisticRegression(random_state=0))])

model = pipeline.fit(X_train, y_train)
with open('LogisticRegression.pickle', 'wb') as f:
    pickle.dump(model, f)

ytest = np.array(y_test)

# confusion matrix and classification report(precision, recall, F1-score)
print(classification_report(ytest, model.predict(X_test)))
print(confusion_matrix(ytest, model.predict(X_test)))

matrix_count_log_1 = confusion_matrix(ytest, model.predict(X_test))
sns.heatmap(matrix_count_log_1, annot=True, fmt="d")
plt.title('Confusion Matrix of Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(ytest, model.predict(X_test)))

from yellowbrick.classifier import ROCAUC

def plot_ROC_curve(model, xtrain, ytrain, xtest, ytest):

    # Creating visualization with the readable labels
    visualizer = ROCAUC(model, encoder={0: 'Negative', 
                                        1: 'Neutral', 
                                        2: 'Positive'})
                                        
    # Fitting to the training data first then scoring with the test data                                    
    visualizer.fit(xtrain, ytrain)
    visualizer.score(xtest, ytest)
    visualizer.show()
    
    return visualizer

X_train2 = vectorizer.fit_transform(X_train)
X_train2 = X_train2.toarray()

X_test2 = vectorizer.transform(X_test)
X_test2 = X_test2.toarray()

"""## 6.2 Random Forest
Let's create a model according to 3 different methods and compare their success.

<!-- k-Nearest Neighbors.
Decision Trees.
Naive Bayes.
Random Forest.
Gradient Boosting. -->
"""

forest = RandomForestClassifier() 
forest = forest.fit(X_train2, y_train)

from sklearn.metrics import accuracy_score

predictions2 = forest.predict(X_test2) 
print("Accuracy: ", accuracy_score(y_test, predictions2))

matrix_count_log_3 = confusion_matrix(ytest, forest.predict(X_test2))
sns.heatmap(matrix_count_log_3, annot=True, fmt="d")
plt.title('Confusion Matrix of RandomForest Classifier')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(ytest, forest.predict(X_test2)))

"""## 6.3 Baseline Model using MultinomialNB"""

model_nb=MultinomialNB(alpha = 0.1)
model_nb = model_nb.fit(X_train2, y_train)

predictions3 = model_nb.predict(X_test2) 
print("Accuracy: ", accuracy_score(y_test, predictions3))

matrix_count_log_4 = confusion_matrix(ytest, model_nb.predict(X_test2))
sns.heatmap(matrix_count_log_4, annot=True, fmt="d")
plt.title('Confusion Matrix of MultinomialNB Classifier')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(ytest, model_nb.predict(X_test2)))

"""## 6.4 BERT

Converting text into tokens
"""

df.head()

df = df.sample(frac=1)
df.dropna(inplace=True)

n=int(np.floor(df.shape[0]*0.8))
train= df[:n][['text', 'sentiment_label']]
test= df[n:][['text', 'sentiment_label']]

train.head()

model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

train["sentiment"] = train["sentiment_label"].map({"Neutral":0, "Positive":0.5, "Negative": 1})
test["sentiment"] = test["sentiment_label"].map({"Neutral":0, "Positive":0.5, "Negative": 1})

train.head()

def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN):
  train_InputExamples = train.apply(lambda x : InputExample(guid=None,
                                                            text_a=x[DATA_COLUMN],
                                                            text_b=None,
                                                            label=x[LABEL_COLUMN]), axis=1)
  
  validation_InputExamples = test.apply(lambda x : InputExample(guid=None,
                                                            text_a=x[DATA_COLUMN],
                                                            text_b=None,
                                                            label=x[LABEL_COLUMN]), axis=1)

  return train_InputExamples, validation_InputExamples


def convert_examples_to_tf_dataset(examples, tokenizer, max_length=100):
  features = []

  for e in examples:
    input_dict = tokenizer.encode_plus(
        e.text_a,
        add_special_tokens=True,
        max_length=max_length,
        return_token_type_ids=True,
        return_attention_mask=True,
        pad_to_max_length=True,
        truncation=True
    )

    input_ids, token_type_ids, attention_mask = (input_dict["input_ids"], 
                                                 input_dict["token_type_ids"],
                                                 input_dict["attention_mask"])
    features.append(
        InputFeatures(
            input_ids=input_ids, 
            attention_mask=attention_mask, 
            token_type_ids=token_type_ids, 
            label=e.label
        )
    )

  def gen():
    for f in features:
      yield (
          {
              "input_ids":f.input_ids,
              "attention_mask":f.attention_mask,
              "token_type_ids":f.token_type_ids
          },
          f.label)

  return tf.data.Dataset.from_generator(
      gen,
      ({"input_ids":tf.int32, "attention_mask":tf.int32, "token_type_ids":tf.int32}, tf.int64),
      (
          {
            "input_ids":tf.TensorShape([None]),
           "attention_mask":tf.TensorShape([None]),
           "token_type_ids":tf.TensorShape([None])
          },
       tf.TensorShape([]),
      ),
      
  )

DATA_COLUMN = "text"
LABEL_COLUMN = "sentiment"

train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)

train_InputExamples.dropna(inplace=True)
validation_InputExamples.dropna(inplace=True)

train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)
train_data = train_data.shuffle(100).batch(32).repeat(2)

validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)
validation_data = validation_data.batch(32)

# configure bert
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])

model.fit(train_data, epochs=2, validation_data=validation_data)

import os

checkpoint_path = "bert_323_9096.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

model.save_weights(checkpoint_path.format(epoch=2))

model.summary()

"""## 6.4 XLNET"""

import pandas as pd

df2 = pd.read_csv("content/data/vax_tweets_sentiment.csv",engine='python')
df2.head()

df2.shape

df2 = df2.drop(['id'], axis=1)

lemmatizer = WordNetLemmatizer()

def tweets_cleaning(df, text_col="text", emoji_map={}):
  
  # demojify
  def replace_emoji(s):
    if s is None:
      return ""
    for word, replacement in emoji_map.items():
      s = s.replace(word, " "+replacement+" ")
    return s

  df["original_text"] = df[text_col].copy()
  df[text_col] = df[text_col].apply(replace_emoji)


  # Remove twitter handles
  df[text_col] = df[text_col].apply(lambda x:re.sub('@[^\s]+','',str(x)))
    
  # Remove URLs
  df[text_col] = df[text_col].apply(lambda x:re.sub(r"http\S+", "", str(x)))

  # Remove hashtags
  df[text_col] = df[text_col].apply(lambda x:re.sub(r'\B#\S+','',str(x)))

  # Lower casing
  df[text_col] = df[text_col].str.lower()

  # remove numbers and special characters
  df[text_col] = df[text_col].apply(lambda x : re.sub(r"[^a-zA-Z ]", "", x))

  df = df[df[text_col].apply(lambda x : len(x))>2]
  
  # Remove stopwords
  def remove_stopwords(text):
    STOPWORDS = set(stopwords.words('english'))
    update_words = ['covid','#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', 
                    '#covid19', '#covid_19', '#epitwitter', '#ihavecorona', 'amp', 'coronavirus', 'covid19']
    STOPWORDS.update(update_words)
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])

  def lemmatize(text):
    word_list = nltk.word_tokenize(text)
    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
    if text!=lemmatized_output:
      print(text,"===",lemmatized_output)
    return lemmatized_output

  
  df[text_col] = df[text_col].apply(lambda text: remove_stopwords(text))

  #df[text_col] = df[text_col].apply(lambda text: lemmatize(text))

  # Single character or double space removal
  df[text_col] = df[text_col].apply(lambda x:re.sub(r'\s+[a-zA-Z]\s+', '', str(x)))
  df[text_col] = df[text_col].apply(lambda x:re.sub(r'\s+', ' ', str(x), flags=re.I))

  return df[df[text_col]!=''] # remove none

df2 = tweets_cleaning(df2, emoji_map=emoji_map)

df2 = df2.drop('Unnamed: 0', axis = 1)

df2_base_learner = df2[:n]
print("df shape is: ", df2.shape)
print("df_base_learner shape is: ",df2_base_learner.shape)

X = df2_base_learner.drop('sentiment', axis = 1)
y = df2_base_learner['sentiment']

tokenizer2 = BertTokenizer.from_pretrained("bert-base-uncased")

x2=int(np.floor(df2_base_learner.shape[0]*0.8))
train2 = df2_base_learner[:x2][['sentiment', 'original_text']]
test2 = df2_base_learner[x2:][['sentiment', 'original_text']]

xlnet_model = 'xlnet-base-cased'
xlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)

train2["sentiment_label"] = train2["sentiment"].map({"neutral":0, "positive":0.5, "negative": 1})
test2["sentiment_label"] = test2["sentiment"].map({"neutral":0, "positive":0.5, "negative": 1})

def convert_data_to_examples(train, DATA_COLUMN, LABEL_COLUMN):
  train_InputExamples = train.apply(lambda x : InputExample(guid=None,
                                                            text_a=x[DATA_COLUMN],
                                                            text_b=None,
                                                            label=x[LABEL_COLUMN]), axis=1)
  return train_InputExamples

def convert_examples_to_tf_dataset(examples, tokenizer):
  features = []

  for e in examples:
    input_dict = tokenizer.encode_plus(
        e.text_a,
        add_special_tokens=True,
        max_length=len(examples),
        return_token_type_ids=True,
        return_attention_mask=True,
        pad_to_max_length=True,
        truncation=True
    )

    input_ids, token_type_ids, attention_mask = (input_dict["input_ids"], input_dict["token_type_ids"],
                                                 input_dict["attention_mask"])
  
  return np.array(input_ids), np.array(attention_mask) 

DATA_COLUMN = "original_text"
LABEL_COLUMN = "sentiment_label"

def create_model_xlnet(xlnet_model):
    word_inputs = tf.keras.Input(shape=(None, ), name='word_inputs', dtype='int32')

    
    xlnet = TFXLNetModel.from_pretrained(xlnet_model)
    xlnet_encodings = xlnet(word_inputs)[0]

    # Collect last step from last hidden state (CLS)
    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)
    
    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)
     
    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(doc_encoding)

    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])
    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])

    return model

xlnet = create_model_xlnet(xlnet_model)
xlnet.summary()

train_InputExamples2 = convert_data_to_examples(train2, DATA_COLUMN, LABEL_COLUMN)
test_InputExamples2 = convert_data_to_examples(test2, DATA_COLUMN, LABEL_COLUMN)
train_input_ids, train_attention_masks = convert_examples_to_tf_dataset(list(train_InputExamples2), tokenizer)
test_input_ids, test_attention_masks = convert_examples_to_tf_dataset(list(test_InputExamples2), tokenizer)

targets = train2['sentiment_label'].values
targets_y = test2['sentiment_label'].values
targets= np.array(targets).reshape(-1, 1)

targets.shape

train_input_ids = np.array(train_input_ids).reshape(-1, 1)
train_input_ids

tf.config.run_functions_eagerly(True)
history_xl = xlnet.fit(train_input_ids, targets, epochs=4, batch_size=64)

targets_y.shape

"""## 6.6 Linear SVC"""

!pip install ktrain

import ktrain
from ktrain import text

df2 = df2.dropna()

X = df2.drop('sentiment', axis = 1)
Y = pd.DataFrame(df2['sentiment'])
Y["sentiment_label"] = Y["sentiment"].map({"neutral":0, "positive":0.5, "negative": 1})

Y = Y.drop('sentiment', axis = 1)

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=0)
X_train.shape
Y_train.shape

from sklearn.feature_extraction.text import CountVectorizer

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X_train['text'])
X_test_counts = count_vect.fit_transform(X_test['text'])

X_train_counts.toarray()

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer()
X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)
X_train_tfidf.shape

print(Y.shape)

from sklearn import svm
linear = svm.LinearSVC
svmmodel = linear().fit(X_train_tfidf, Y_train.astype(int))

from sklearn.pipeline import Pipeline
text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('svmmodel', linear())])
text_clf = text_clf.fit(X_train['text'].squeeze().ravel(), Y_train.astype('int').squeeze().ravel())

predicted = text_clf.predict(X_test['text'].values.astype('U'))
text_clf.score(X_test['text'].values.astype('U'), Y_test.astype('int'))

y_pred = pd.Series(text_clf.predict(X_test['text']))

from sklearn import metrics
import matplotlib.pyplot as plt

#calculate AUC of model
auc = metrics.roc_auc_score(Y_test.astype('int'), y_pred.astype('int'))

#print AUC score
print(auc)

metrics.plot_roc_curve(text_clf, X_test['text'], Y_test.astype('int'))

metrics.plot_roc_curve(text_clf, X_test['text'], Y_test.astype('int'))

linear_svc = svm.LinearSVC(verbose=0)
linear_svc = linear_svc.fit(X_train2, y_train)

predictions4 = linear_svc.predict(X_test2) 
print("Accuracy: ", accuracy_score(y_test, predictions4))

matrix_count_log_5 = confusion_matrix(ytest, linear_svc.predict(X_test2))
sns.heatmap(matrix_count_log_5, annot=True, fmt="d")
plt.title('Confusion Matrix of LinearSVC Classifier')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(ytest, linear_svc.predict(X_test2)))